{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression,scale_coords\n",
    "from utils.torch_utils import  TracedModel\n",
    "from weight.rec.model import CNN\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hank/.local/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgsz = 416\n",
    "conf_thres=0.7\n",
    "iou_thres=0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights_crop = ['weight/crop/best.pt']\n",
    "model_crop = attempt_load(weights_crop, map_location=device) \n",
    "stride_crop = int(model_crop.stride.max())\n",
    "model_crop = TracedModel(model_crop, device, imgsz)\n",
    "model_crop.half()\n",
    "model_crop(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model_crop.parameters())))\n",
    "model_crop.eval()\n",
    "\n",
    "imgsz = 416\n",
    "conf_thres=0.7\n",
    "iou_thres=0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights_seg = ['weight/seg/best.pt']\n",
    "model_seg = attempt_load(weights_seg, map_location=device) \n",
    "stride_seg = int(model_seg.stride.max())\n",
    "model_seg = TracedModel(model_seg, device, imgsz)\n",
    "model_seg.half()\n",
    "model_seg(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model_seg.parameters())))\n",
    "model_crop.eval()\n",
    "\n",
    "LABEL_DICT = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'T', 29: 'U', 30: 'V', 31: 'W', 32: 'X', 33: 'Y', 34: 'Z', 35: '2', 36: '3', 37: '6', 38: '9', 39: 'B', 40: 'K', 41: 'R'}\n",
    "model_rec = CNN()\n",
    "model_rec.load_state_dict(torch.load(\"weight/rec/model.pt\"))\n",
    "model_rec.eval()\n",
    "convert_tensor = transforms.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_yolo(model,stride, origin_img):\n",
    "    img = letterbox(origin_img, imgsz, stride=stride)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half()\n",
    "    img /= 255.0 \n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
    "        pred = model(img, augment=True)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes=None, agnostic=False)[0]\n",
    "    pred = [pred[pred[:, 0].sort(descending=True)[1]]]\n",
    "    \n",
    "    output = {\n",
    "        \"box\":list(),\n",
    "        \"image\":list(),\n",
    "    }\n",
    "\n",
    "    for i, det in enumerate(pred):\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], origin_img.shape).round()\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                c1, c2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))\n",
    "                output[\"box\"].append([c1,c2])\n",
    "                output[\"image\"].append(origin_img[c1[1]:c2[1], c1[0]:c2[0]])\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rec(img):\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    ret, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV) \n",
    "    img_tensor = convert_tensor(img)\n",
    "    img_tensor = torch.unsqueeze(img_tensor, dim = 1)\n",
    "    predic_label, predict_img = model_rec(img_tensor)\n",
    "    predict = torch.max(predic_label, 1)[1].item()\n",
    "    # print(\"predict : \" + str(LABEL_DICT[predict]))\n",
    "    return str(LABEL_DICT[predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(origin_img):\n",
    "    # crop the car plate license\n",
    "    crop_output = predict_yolo(model_crop, stride_crop,origin_img)\n",
    "    print(\"crop done !!!\")\n",
    "\n",
    "    if crop_output[\"box\"] != []:\n",
    "        labels = list()\n",
    "        # segment the car plate license\n",
    "        for index, crop_img in enumerate(crop_output[\"image\"]):\n",
    "            if index > 6:\n",
    "                break\n",
    "            seg_output = predict_yolo(model_seg, stride_seg,crop_img)\n",
    "            print(\"seg done !!!\")\n",
    "\n",
    "            # predict the word\n",
    "            rec_output = \"\"\n",
    "            for seg_img in seg_output[\"image\"]:\n",
    "                rec_output += predict_rec(seg_img)\n",
    "            labels.append(rec_output)\n",
    "        return crop_output[\"box\"] , labels\n",
    "    else:\n",
    "        return [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop done !!!\n",
      "seg done !!!\n"
     ]
    }
   ],
   "source": [
    "img = \"image/test1.jpg\"\n",
    "origin_img = cv2.imread(img)\n",
    "start = time.time()\n",
    "boxes, labels = predict(origin_img)\n",
    "\n",
    "\n",
    "for box, label in zip(boxes, labels):\n",
    "    x1, y1, x2, y2 = box[0][0], box[0][1], box[1][0], box[1][1]\n",
    "    cv2.rectangle(origin_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    cv2.putText(origin_img, \"{}\".format(label), (x1, y1 - 13), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4)\n",
    "    cv2.imwrite('image/test1_output.jpg', origin_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
